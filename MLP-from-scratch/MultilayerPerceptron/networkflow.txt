# Shape [[(input_dim, hidden_in_dim)], [biases]]
hidden1 = [
[.2, .8],
[.3, .7],
[1, 1]
]

# Shape [[(hidden_in_dim, output_in_dim)], [biases]]
output = [[[.1, .25], [.5, .05], [.2, .7]], [1, 1, 1]]

x1 = .24
x2 = .98

hidden_input_0 = x1 * i[0][0] + x2 * i[1][0] + 1 * b1[0]
hidden_input_1 = x1 * i[0][1] + x2 * i[1][1] + 1 * b1[1]

output_input_0 = h_i_0 * h[0][0] + h_i_1 * h[1][0] + 1 * b2[0]
output_input_1 = h_i_0 * h[0][1] + h_i_1 * h[1][1] + 1 * b2[1]
output_input_2 = h_i_0 * h[0][2] + h_i_1 * h[1][2] + 1 * b3[2]

# ------
#  Flow
# ------

# Legacy way

curr_layer = hidden1

curr_lay_in = input_features
nxt_lay_in = []

while curr_layer != null

	1. For each curr_lay_in (i)
		2. For each node of current layer (j)
			3. Add:
				nxt_lay_in[j] += curr_lay_in[i] * curr_layer[0][i][j] + curr_layer[1][j]		

	# Apply layer activation function to nxt_lay_in
		
	curr_layer = nextLayer()
	curr_lay_in = nxt_lay_in # Deep clone it
	nxt_lay_in = []

# curr_lay_in is the network output

